{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel\nimport emoji\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nimport random\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# Dataset\ndata = [\n    {\"sentence\": \"Oh, I just LOVE Mondays! ✨\", \"polarity\": -1},\n    {\"sentence\": \"My internet is so fast, it takes only 5 minutes to load a single image. 🐌\", \"polarity\": -1},\n    {\"sentence\": \"This meeting is SO productive. 😴\", \"polarity\": -1},\n    {\"sentence\": \"My favorite activity is listening to my neighbor's dog bark at 3 AM. 🎶\", \"polarity\": -1},\n    {\"sentence\": \"Nothing like spending my Friday night debugging code. 🎉\", \"polarity\": -1},\n    {\"sentence\": \"I'm so glad I wore white pants today. It's only raining mud. 👍\", \"polarity\": -1},\n    {\"sentence\": \"My hair looks AMAZING today. (After I spent an hour trying to fix it) Medusa Head\", \"polarity\": -1},\n    {\"sentence\": \"This traffic is absolutely delightful. 🚗💨🤬\", \"polarity\": -1},\n    {\"sentence\": \"I can't wait to hear this exciting news. 🙄\", \"polarity\": -1},\n    {\"sentence\": \"I'm thrilled that my alarm didn't go off this morning. Now I'm definitely not late. ⏰\", \"polarity\": -1},\n    {\"sentence\": \"Group projects are the BEST. Everyone contributes equally, right? 🤡\", \"polarity\": -1},\n    {\"sentence\": \"I love it when people tell me what to do. 🥰\", \"polarity\": -1},\n    {\"sentence\": \"I'm so good at adulting. I haven't paid my bills in weeks. 💯\", \"polarity\": -1},\n    {\"sentence\": \"This weather is perfect for a picnic. (It's hailing) 🧺\", \"polarity\": -1},\n    {\"sentence\": \"I'm so excited to work overtime this weekend. 🥳\", \"polarity\": -1},\n    {\"sentence\": \"I just LOVE it when my phone dies right before I need to make an important call. 📱💀\", \"polarity\": -1},\n    {\"sentence\": \"My bank account is overflowing with money. 🤑 (Not really)\", \"polarity\": -1},\n    {\"sentence\": \"This cake is so delicious, I could eat the whole thing. 🤮 (It's actually terrible)\", \"polarity\": -1},\n    {\"sentence\": \"Cleaning my room is my favorite hobby. ✨\", \"polarity\": -1},\n    {\"sentence\": \"I always follow the recipe exactly. That's why my food looks like this. 🔥\", \"polarity\": -1},\n    {\"sentence\": \"I'm having so much fun writing this report. 📝😴\", \"polarity\": -1},\n    {\"sentence\": \"This is fine. 🔥\", \"polarity\": -1},\n    {\"sentence\": \"I'm not stressed at all. 😌\", \"polarity\": -1},\n    {\"sentence\": \"This is my favorite song. 🙉\", \"polarity\": -1},\n    {\"sentence\": \"I'm so glad I left my umbrella at home. 🌧️\", \"polarity\": -1},\n    {\"sentence\": \"I love getting stuck in traffic. 🚗😣\", \"polarity\": -1},\n    {\"sentence\": \"This is going to be a great day. 😒\", \"polarity\": -1},\n    {\"sentence\": \"I'm so excited for this exam. 📚😭\", \"polarity\": -1},\n    {\"sentence\": \"I can't wait to do laundry. 🧺🙄\", \"polarity\": -1},\n    {\"sentence\": \"I love waking up early. ⏰😣\", \"polarity\": -1},\n    {\"sentence\": \"The weather is pleasant today.\", \"polarity\": 0},\n    {\"sentence\": \"The train arrives at 2 PM.\", \"polarity\": 0},\n    {\"sentence\": \"Water boils at 100 degrees Celsius.\", \"polarity\": 0},\n    {\"sentence\": \"The Earth revolves around the Sun.\", \"polarity\": 0},\n    {\"sentence\": \"Paris is the capital of France.\", \"polarity\": 0},\n    {\"sentence\": \"There are 24 hours in a day.\", \"polarity\": 0},\n    {\"sentence\": \"The library opens at 9 AM.\", \"polarity\": 0},\n    {\"sentence\": \"The movie starts at 7 PM.\", \"polarity\": 0},\n    {\"sentence\": \"Today is Wednesday.\", \"polarity\": 0},\n    {\"sentence\": \"I'm so happy to be here! 😄\", \"polarity\": 1},\n    {\"sentence\": \"This is amazing! 🤩\", \"polarity\": 1},\n    {\"sentence\": \"I love this! ❤️\", \"polarity\": 1},\n    {\"sentence\": \"This is the best day ever! 🎉\", \"polarity\": 1},\n    {\"sentence\": \"I'm feeling great! 😊\", \"polarity\": 1},\n    {\"sentence\": \"This is fantastic! ✨\", \"polarity\": 1},\n    {\"sentence\": \"I'm so grateful for this! 🙏\", \"polarity\": 1},\n    {\"sentence\": \"This is wonderful! 😍\", \"polarity\": 1},\n    {\"sentence\": \"I'm so lucky! 🍀\", \"polarity\": 1},\n    {\"sentence\": \"This is perfect! 👌\", \"polarity\": 1}\n]\n\n# Convert polarity to labels: -1 -> 0 (negative), 0 -> 1 (neutral), 1 -> 2 (positive)\nfor item in data:\n    item['label'] = item['polarity'] + 1\n\n# Initialize BERT tokenizer\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Extract all unique emojis from the dataset\nunique_emojis = set()\nfor item in data:\n    emojis = [char for char in item['sentence'] if char in emoji.EMOJI_DATA]\n    unique_emojis.update(emojis)\n\n# Add emojis directly as tokens to the tokenizer\nbert_tokenizer.add_tokens(list(unique_emojis))\n\n# Custom Dataset\nclass SentimentDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        sentence = item['sentence']\n        label = item['label']\n\n        encoding = self.tokenizer(\n            sentence,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Split dataset\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\ntrain_dataset = SentimentDataset(train_data, bert_tokenizer)\ntest_dataset = SentimentDataset(test_data, bert_tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\n# Attention Layer\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(Attention, self).__init__()\n        self.attention = nn.Linear(hidden_dim * 2, 1)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, lstm_output):\n        attention_scores = self.attention(lstm_output).squeeze(-1)\n        attention_weights = self.softmax(attention_scores)\n        context_vector = torch.bmm(attention_weights.unsqueeze(1), lstm_output).squeeze(1)\n        return context_vector, attention_weights\n\n# Sentiment Analysis Model\nclass SentimentModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=768, hidden_dim=256, output_dim=3, dropout=0.3):\n        super(SentimentModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.attention = Attention(hidden_dim)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_ids, attention_mask):\n        embedded = self.embedding(input_ids)\n        lstm_output, _ = self.lstm(embedded)\n        context_vector, attention_weights = self.attention(lstm_output)\n        output = self.dropout(context_vector)\n        output = self.fc(output)\n        return output\n\n# Initialize model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SentimentModel(vocab_size=len(bert_tokenizer), embedding_dim=768, hidden_dim=256, output_dim=3)\nmodel.to(device)\n\n# Load pretrained BERT embeddings for the embedding layer\nbert_model = BertModel.from_pretrained('bert-base-uncased')\nwith torch.no_grad():\n    # Copy BERT's pretrained embeddings for the original vocabulary\n    model.embedding.weight[:bert_tokenizer.vocab_size] = bert_model.embeddings.word_embeddings.weight\n    # Initialize embeddings for new emoji tokens with random values\n    num_new_tokens = len(bert_tokenizer) - bert_tokenizer.vocab_size\n    if num_new_tokens > 0:\n        new_embeddings = torch.randn(num_new_tokens, 768) * 0.02  # Small random init\n        model.embedding.weight[bert_tokenizer.vocab_size:] = new_embeddings\n\n# Training setup\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=2e-5)\n\n# Training loop\ndef train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        for batch in train_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n        \n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}')\n\n# Evaluation\ndef evaluate_model(model, test_loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(input_ids, attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    print(f'Test Accuracy: {accuracy:.4f}')\n    print(f'Test F1 Score: {f1:.4f}')\n\n# Run training and evaluation\ntrain_model(model, train_loader, criterion, optimizer, num_epochs=10)\nevaluate_model(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:35:49.373779Z","iopub.execute_input":"2025-05-03T13:35:49.374307Z","iopub.status.idle":"2025-05-03T13:36:19.208059Z","shell.execute_reply.started":"2025-05-03T13:35:49.374285Z","shell.execute_reply":"2025-05-03T13:36:19.207262Z"}},"outputs":[{"name":"stderr","text":"2025-05-03 13:36:01.215225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746279361.369388      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746279361.413600      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d59e25c3c5c04a03b376389cba5631a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08dd896f15824a2abac48d93a8aefb5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e5df02c41594a588813855160ba9658"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3e044905dd94ab0b01583385386f18a"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f342eaea0a544b16af30e9df0ca7998b"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/10, Loss: 1.1060\nEpoch 2/10, Loss: 1.1001\nEpoch 3/10, Loss: 1.1005\nEpoch 4/10, Loss: 1.0972\nEpoch 5/10, Loss: 1.0909\nEpoch 6/10, Loss: 1.0869\nEpoch 7/10, Loss: 1.0861\nEpoch 8/10, Loss: 1.0803\nEpoch 9/10, Loss: 1.0758\nEpoch 10/10, Loss: 1.0758\nTest Accuracy: 0.6000\nTest F1 Score: 0.4500\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def predict_sentiment(text, tokenizer, model, device, max_length=128):\n    model.eval()\n    # Tokenize input text\n    encoding = tokenizer(\n        text,\n        add_special_tokens=True,\n        max_length=max_length,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt'\n    )\n    \n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask)\n        _, pred = torch.max(outputs, dim=1)\n    \n    # Map prediction to sentiment label\n    sentiment_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n    return sentiment_map[pred.item()]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:38:38.932031Z","iopub.execute_input":"2025-05-03T13:38:38.932675Z","iopub.status.idle":"2025-05-03T13:38:38.937765Z","shell.execute_reply.started":"2025-05-03T13:38:38.932649Z","shell.execute_reply":"2025-05-03T13:38:38.937131Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"example_text = \"happy,love joy\"\nsentiment = predict_sentiment(example_text, bert_tokenizer, model, device)\nprint(f\"Sentiment for '{example_text}': {sentiment}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:50:44.574569Z","iopub.execute_input":"2025-05-03T13:50:44.574864Z","iopub.status.idle":"2025-05-03T13:50:44.584381Z","shell.execute_reply.started":"2025-05-03T13:50:44.574844Z","shell.execute_reply":"2025-05-03T13:50:44.583631Z"}},"outputs":[{"name":"stdout","text":"Sentiment for 'happy,love joy': Negative\n","output_type":"stream"}],"execution_count":13}]}